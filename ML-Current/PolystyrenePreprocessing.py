# Exported my preprocessing jupyter notebook into a python script and added more comments. 

# %%
import pandas as pd
import numpy as np

# %%
# Note this file is not a direct copy of LCCC on One-drive.
# I switched the commas to semicolons in the solvents column 
# and removed an invisible character that plagued the injection volume column- I believe.

original_file = r"C:\Users\micha\OneDrive\Documents\GitHub\LCCC-ML\LCCCSeed-ML-2-2-25-RemovedCommasFromSolvents.xlsx"

original_data = pd.read_excel(original_file)

# %%
# This portion prints all the missing statistics without dropping anything.
# The script is redundant as to verify that only columns with Percentage missing > 50% are dropped. 

dropped_columns = []
column_notmissing = []
column_missing = []
total_rows = len(original_data)

for column in original_data.columns:
    missing_count = original_data[column].isna().sum()

    missing_percentage = (missing_count / total_rows) * 100

    print(f"Column: {column}")
    print(f" Missing Values: {missing_count}")
    print(f" Percentage Missing: {missing_percentage:.2f}%\n")

# %%
# This portion prints all the missing statistics THEN drops anything missing greater than 50% Percentage missing.

dropped_columns = []
column_notmissing = []
column_missing = []
total_rows = len(original_data)


for column in original_data.columns:
    missing_count = original_data[column].isna().sum()

    missing_percentage = (missing_count / total_rows) * 100

    print(f"Column: {column}")
    print(f" Missing Values: {missing_count}")
    print(f" Percentage Missing: {missing_percentage:.2f}%\n")
    
    if (original_data[column].isna().sum() / len(original_data)) * 100 >= 50:
        dropped_columns.append(column)

# %%
# Print columns that are dropped due to missing percentage are tracked and printed.
print(dropped_columns)

# %%
# Relabeling the data frame that contains only the columns that are kept.

data = original_data.drop(columns=dropped_columns, axis=1)

# %%
#data

# %%
# List of columns still in dataframe - used to manually specify some of the columns that were experimental data.
# Non experimental data is then dropped from the dataframe.

data.columns

# %%
Not_needed_columns = ['Entry_ID', 'Include in final draft?', 'Verified', 'Reliability', 'Indirect Reliability score (Special Penalty: -2)', 'Low Reliability Score', 'Missing Confirmed Critical Range',
           'Missing Particle Diameter of Stationary Phase', 'Missing Column Dimensions of Stationary Phase', 'Missing Column Dimensions of Stationary Phase', 'Unknown Stationary Phase phase (normal, reverse, etc.)',
           'Unable to Access Paper (by our team at the point of this publication)', 'Had to be Translated to English', 'Critical Conditions Present in Only One Paper', 'Reference', 'Alternate Polymer Names', 'Manufacturer', 'Figure(s)', 'Column dimension (cm)', 'Solvent Ratio Unit', 'Missing Pore Size of Stationary Phase']

# Non experimental data is dropped from dataframe.
data.drop(columns=Not_needed_columns, axis=1, inplace=True)

# %%

data.columns

# %%
#data

# %%
# Here I used a nifty visual studio code extension Data Wrangler to filter the data to only polystyrene entries.
# It generates an intuitive excel like interface where actions done and then exported as script.
# Sorry if the variable naming scheme go a bit wild - every time I use this extension, it creates a new variable to store changes.

"""
Cell generated by Data Wrangler.
"""
def clean_data(data):
    # Filter rows based on column: 'Polymer'
    data = data[data['Polymer'] == "Polystyrene"]
    return data

data_clean = clean_data(data.copy())
#data_clean.head()

# %%
data_clean

# %%
# Reading in the Solvent-RDkit-Descriptors-2-2-25.xlsx file which contains a list of all the solvents whose SMILES I could use RDkit to generate descriptors from.
# There are a couple very weird solvents that I am still having trouble with - one example is Deuterated Acetone. I do not believe any solvents issues present with the polystyrene subset.

# Note that the excel sheet contains both the molecular descriptors and the morgan finger print descriptors. 
# Molecular descriptors are ~ 200 columns while MorganFinger prints are ~ 1000 columns attached to the end.


solvent_path = r"C:\Users\micha\OneDrive\Documents\GitHub\LCCC-ML\Solvent-RDkit-Descriptors-2-2-25.xlsx"
solvent_desc = pd.read_excel(solvent_path)

# %%
# Only 49 solvents had their descriptors generated.

solvent_desc

# %%
# Here I used the Data Wrangler extension to drop all of the morgan finger print descriptors from the descriptors excel sheet.

"""
Cell generated by Data Wrangler.
"""
def clean_data(solvent_desc):
    # Drop columns: 'FP_bit_0', 'FP_bit_1' and 1022 other columns
    solvent_desc = solvent_desc.drop(columns=['FP_bit_0', 'FP_bit_1', 'FP_bit_2', 'FP_bit_3', 'FP_bit_4', 'FP_bit_5', 'FP_bit_6', 'FP_bit_7', 'FP_bit_8', 'FP_bit_9', 'FP_bit_10', 'FP_bit_11', 'FP_bit_12', 'FP_bit_13', 'FP_bit_14', 'FP_bit_15', 'FP_bit_16', 'FP_bit_17', 'FP_bit_18', 'FP_bit_19', 'FP_bit_20', 'FP_bit_21', 'FP_bit_22', 'FP_bit_23', 'FP_bit_24', 'FP_bit_25', 'FP_bit_26', 'FP_bit_27', 'FP_bit_28', 'FP_bit_29', 'FP_bit_30', 'FP_bit_31', 'FP_bit_32', 'FP_bit_33', 'FP_bit_34', 'FP_bit_35', 'FP_bit_36', 'FP_bit_37', 'FP_bit_38', 'FP_bit_39', 'FP_bit_40', 'FP_bit_41', 'FP_bit_42', 'FP_bit_43', 'FP_bit_44', 'FP_bit_45', 'FP_bit_46', 'FP_bit_47', 'FP_bit_48', 'FP_bit_49', 'FP_bit_50', 'FP_bit_51', 'FP_bit_52', 'FP_bit_53', 'FP_bit_54', 'FP_bit_55', 'FP_bit_56', 'FP_bit_57', 'FP_bit_58', 'FP_bit_59', 'FP_bit_60', 'FP_bit_61', 'FP_bit_62', 'FP_bit_63', 'FP_bit_64', 'FP_bit_65', 'FP_bit_66', 'FP_bit_67', 'FP_bit_68', 'FP_bit_69', 'FP_bit_70', 'FP_bit_71', 'FP_bit_72', 'FP_bit_73', 'FP_bit_74', 'FP_bit_75', 'FP_bit_76', 'FP_bit_77', 'FP_bit_78', 'FP_bit_79', 'FP_bit_80', 'FP_bit_81', 'FP_bit_82', 'FP_bit_83', 'FP_bit_84', 'FP_bit_85', 'FP_bit_86', 'FP_bit_87', 'FP_bit_88', 'FP_bit_89', 'FP_bit_90', 'FP_bit_91', 'FP_bit_92', 'FP_bit_93', 'FP_bit_94', 'FP_bit_95', 'FP_bit_96', 'FP_bit_97', 'FP_bit_98', 'FP_bit_99', 'FP_bit_100', 'FP_bit_101', 'FP_bit_102', 'FP_bit_103', 'FP_bit_104', 'FP_bit_105', 'FP_bit_106', 'FP_bit_107', 'FP_bit_108', 'FP_bit_109', 'FP_bit_110', 'FP_bit_111', 'FP_bit_112', 'FP_bit_113', 'FP_bit_114', 'FP_bit_115', 'FP_bit_116', 'FP_bit_117', 'FP_bit_118', 'FP_bit_119', 'FP_bit_120', 'FP_bit_121', 'FP_bit_122', 'FP_bit_123', 'FP_bit_124', 'FP_bit_125', 'FP_bit_126', 'FP_bit_127', 'FP_bit_128', 'FP_bit_129', 'FP_bit_130', 'FP_bit_131', 'FP_bit_132', 'FP_bit_133', 'FP_bit_134', 'FP_bit_135', 'FP_bit_136', 'FP_bit_137', 'FP_bit_138', 'FP_bit_139', 'FP_bit_140', 'FP_bit_141', 'FP_bit_142', 'FP_bit_143', 'FP_bit_144', 'FP_bit_145', 'FP_bit_146', 'FP_bit_147', 'FP_bit_148', 'FP_bit_149', 'FP_bit_150', 'FP_bit_151', 'FP_bit_152', 'FP_bit_153', 'FP_bit_154', 'FP_bit_155', 'FP_bit_156', 'FP_bit_157', 'FP_bit_158', 'FP_bit_159', 'FP_bit_160', 'FP_bit_161', 'FP_bit_162', 'FP_bit_163', 'FP_bit_164', 'FP_bit_165', 'FP_bit_166', 'FP_bit_167', 'FP_bit_168', 'FP_bit_169', 'FP_bit_170', 'FP_bit_171', 'FP_bit_172', 'FP_bit_173', 'FP_bit_174', 'FP_bit_175', 'FP_bit_176', 'FP_bit_177', 'FP_bit_178', 'FP_bit_179', 'FP_bit_180', 'FP_bit_181', 'FP_bit_182', 'FP_bit_183', 'FP_bit_184', 'FP_bit_185', 'FP_bit_186', 'FP_bit_187', 'FP_bit_188', 'FP_bit_189', 'FP_bit_190', 'FP_bit_191', 'FP_bit_192', 'FP_bit_193', 'FP_bit_194', 'FP_bit_195', 'FP_bit_196', 'FP_bit_197', 'FP_bit_198', 'FP_bit_199', 'FP_bit_200', 'FP_bit_201', 'FP_bit_202', 'FP_bit_203', 'FP_bit_204', 'FP_bit_205', 'FP_bit_206', 'FP_bit_207', 'FP_bit_208', 'FP_bit_209', 'FP_bit_210', 'FP_bit_211', 'FP_bit_212', 'FP_bit_213', 'FP_bit_214', 'FP_bit_215', 'FP_bit_216', 'FP_bit_217', 'FP_bit_218', 'FP_bit_219', 'FP_bit_220', 'FP_bit_221', 'FP_bit_222', 'FP_bit_223', 'FP_bit_224', 'FP_bit_225', 'FP_bit_226', 'FP_bit_227', 'FP_bit_228', 'FP_bit_229', 'FP_bit_230', 'FP_bit_231', 'FP_bit_232', 'FP_bit_233', 'FP_bit_234', 'FP_bit_235', 'FP_bit_236', 'FP_bit_237', 'FP_bit_238', 'FP_bit_239', 'FP_bit_240', 'FP_bit_241', 'FP_bit_242', 'FP_bit_243', 'FP_bit_244', 'FP_bit_245', 'FP_bit_246', 'FP_bit_247', 'FP_bit_248', 'FP_bit_249', 'FP_bit_250', 'FP_bit_251', 'FP_bit_252', 'FP_bit_253', 'FP_bit_254', 'FP_bit_255', 'FP_bit_256', 'FP_bit_257', 'FP_bit_258', 'FP_bit_259', 'FP_bit_260', 'FP_bit_261', 'FP_bit_262', 'FP_bit_263', 'FP_bit_264', 'FP_bit_265', 'FP_bit_266', 'FP_bit_267', 'FP_bit_268', 'FP_bit_269', 'FP_bit_270', 'FP_bit_271', 'FP_bit_272', 'FP_bit_273', 'FP_bit_274', 'FP_bit_275', 'FP_bit_276', 'FP_bit_277', 'FP_bit_278', 'FP_bit_279', 'FP_bit_280', 'FP_bit_281', 'FP_bit_282', 'FP_bit_283', 'FP_bit_284', 'FP_bit_285', 'FP_bit_286', 'FP_bit_287', 'FP_bit_288', 'FP_bit_289', 'FP_bit_290', 'FP_bit_291', 'FP_bit_292', 'FP_bit_293', 'FP_bit_294', 'FP_bit_295', 'FP_bit_296', 'FP_bit_297', 'FP_bit_298', 'FP_bit_299', 'FP_bit_300', 'FP_bit_301', 'FP_bit_302', 'FP_bit_303', 'FP_bit_304', 'FP_bit_305', 'FP_bit_306', 'FP_bit_307', 'FP_bit_308', 'FP_bit_309', 'FP_bit_310', 'FP_bit_311', 'FP_bit_312', 'FP_bit_313', 'FP_bit_314', 'FP_bit_315', 'FP_bit_316', 'FP_bit_317', 'FP_bit_318', 'FP_bit_319', 'FP_bit_320', 'FP_bit_321', 'FP_bit_322', 'FP_bit_323', 'FP_bit_324', 'FP_bit_325', 'FP_bit_326', 'FP_bit_327', 'FP_bit_328', 'FP_bit_329', 'FP_bit_330', 'FP_bit_331', 'FP_bit_332', 'FP_bit_333', 'FP_bit_334', 'FP_bit_335', 'FP_bit_336', 'FP_bit_337', 'FP_bit_338', 'FP_bit_339', 'FP_bit_340', 'FP_bit_341', 'FP_bit_342', 'FP_bit_343', 'FP_bit_344', 'FP_bit_345', 'FP_bit_346', 'FP_bit_347', 'FP_bit_348', 'FP_bit_349', 'FP_bit_350', 'FP_bit_351', 'FP_bit_352', 'FP_bit_353', 'FP_bit_354', 'FP_bit_355', 'FP_bit_356', 'FP_bit_357', 'FP_bit_358', 'FP_bit_359', 'FP_bit_360', 'FP_bit_361', 'FP_bit_362', 'FP_bit_363', 'FP_bit_364', 'FP_bit_365', 'FP_bit_366', 'FP_bit_367', 'FP_bit_368', 'FP_bit_369', 'FP_bit_370', 'FP_bit_371', 'FP_bit_372', 'FP_bit_373', 'FP_bit_374', 'FP_bit_375', 'FP_bit_376', 'FP_bit_377', 'FP_bit_378', 'FP_bit_379', 'FP_bit_380', 'FP_bit_381', 'FP_bit_382', 'FP_bit_383', 'FP_bit_384', 'FP_bit_385', 'FP_bit_386', 'FP_bit_387', 'FP_bit_388', 'FP_bit_389', 'FP_bit_390', 'FP_bit_391', 'FP_bit_392', 'FP_bit_393', 'FP_bit_394', 'FP_bit_395', 'FP_bit_396', 'FP_bit_397', 'FP_bit_398', 'FP_bit_399', 'FP_bit_400', 'FP_bit_401', 'FP_bit_402', 'FP_bit_403', 'FP_bit_404', 'FP_bit_405', 'FP_bit_406', 'FP_bit_407', 'FP_bit_408', 'FP_bit_409', 'FP_bit_410', 'FP_bit_411', 'FP_bit_412', 'FP_bit_413', 'FP_bit_414', 'FP_bit_415', 'FP_bit_416', 'FP_bit_417', 'FP_bit_418', 'FP_bit_419', 'FP_bit_420', 'FP_bit_421', 'FP_bit_422', 'FP_bit_423', 'FP_bit_424', 'FP_bit_425', 'FP_bit_426', 'FP_bit_427', 'FP_bit_428', 'FP_bit_429', 'FP_bit_430', 'FP_bit_431', 'FP_bit_432', 'FP_bit_433', 'FP_bit_434', 'FP_bit_435', 'FP_bit_436', 'FP_bit_437', 'FP_bit_438', 'FP_bit_439', 'FP_bit_440', 'FP_bit_441', 'FP_bit_442', 'FP_bit_443', 'FP_bit_444', 'FP_bit_445', 'FP_bit_446', 'FP_bit_447', 'FP_bit_448', 'FP_bit_449', 'FP_bit_450', 'FP_bit_451', 'FP_bit_452', 'FP_bit_453', 'FP_bit_454', 'FP_bit_455', 'FP_bit_456', 'FP_bit_457', 'FP_bit_458', 'FP_bit_459', 'FP_bit_460', 'FP_bit_461', 'FP_bit_462', 'FP_bit_463', 'FP_bit_464', 'FP_bit_465', 'FP_bit_466', 'FP_bit_467', 'FP_bit_468', 'FP_bit_469', 'FP_bit_470', 'FP_bit_471', 'FP_bit_472', 'FP_bit_473', 'FP_bit_474', 'FP_bit_475', 'FP_bit_476', 'FP_bit_477', 'FP_bit_478', 'FP_bit_479', 'FP_bit_480', 'FP_bit_481', 'FP_bit_482', 'FP_bit_483', 'FP_bit_484', 'FP_bit_485', 'FP_bit_486', 'FP_bit_487', 'FP_bit_488', 'FP_bit_489', 'FP_bit_490', 'FP_bit_491', 'FP_bit_492', 'FP_bit_493', 'FP_bit_494', 'FP_bit_495', 'FP_bit_496', 'FP_bit_497', 'FP_bit_498', 'FP_bit_499', 'FP_bit_500', 'FP_bit_501', 'FP_bit_502', 'FP_bit_503', 'FP_bit_504', 'FP_bit_505', 'FP_bit_506', 'FP_bit_507', 'FP_bit_508', 'FP_bit_509', 'FP_bit_510', 'FP_bit_511', 'FP_bit_512', 'FP_bit_513', 'FP_bit_514', 'FP_bit_515', 'FP_bit_516', 'FP_bit_517', 'FP_bit_518', 'FP_bit_519', 'FP_bit_520', 'FP_bit_521', 'FP_bit_522', 'FP_bit_523', 'FP_bit_524', 'FP_bit_525', 'FP_bit_526', 'FP_bit_527', 'FP_bit_528', 'FP_bit_529', 'FP_bit_530', 'FP_bit_531', 'FP_bit_532', 'FP_bit_533', 'FP_bit_534', 'FP_bit_535', 'FP_bit_536', 'FP_bit_537', 'FP_bit_538', 'FP_bit_539', 'FP_bit_540', 'FP_bit_541', 'FP_bit_542', 'FP_bit_543', 'FP_bit_544', 'FP_bit_545', 'FP_bit_546', 'FP_bit_547', 'FP_bit_548', 'FP_bit_549', 'FP_bit_550', 'FP_bit_551', 'FP_bit_552', 'FP_bit_553', 'FP_bit_554', 'FP_bit_555', 'FP_bit_556', 'FP_bit_557', 'FP_bit_558', 'FP_bit_559', 'FP_bit_560', 'FP_bit_561', 'FP_bit_562', 'FP_bit_563', 'FP_bit_564', 'FP_bit_565', 'FP_bit_566', 'FP_bit_567', 'FP_bit_568', 'FP_bit_569', 'FP_bit_570', 'FP_bit_571', 'FP_bit_572', 'FP_bit_573', 'FP_bit_574', 'FP_bit_575', 'FP_bit_576', 'FP_bit_577', 'FP_bit_578', 'FP_bit_579', 'FP_bit_580', 'FP_bit_581', 'FP_bit_582', 'FP_bit_583', 'FP_bit_584', 'FP_bit_585', 'FP_bit_586', 'FP_bit_587', 'FP_bit_588', 'FP_bit_589', 'FP_bit_590', 'FP_bit_591', 'FP_bit_592', 'FP_bit_593', 'FP_bit_594', 'FP_bit_595', 'FP_bit_596', 'FP_bit_597', 'FP_bit_598', 'FP_bit_599', 'FP_bit_600', 'FP_bit_601', 'FP_bit_602', 'FP_bit_603', 'FP_bit_604', 'FP_bit_605', 'FP_bit_606', 'FP_bit_607', 'FP_bit_608', 'FP_bit_609', 'FP_bit_610', 'FP_bit_611', 'FP_bit_612', 'FP_bit_613', 'FP_bit_614', 'FP_bit_615', 'FP_bit_616', 'FP_bit_617', 'FP_bit_618', 'FP_bit_619', 'FP_bit_620', 'FP_bit_621', 'FP_bit_622', 'FP_bit_623', 'FP_bit_624', 'FP_bit_625', 'FP_bit_626', 'FP_bit_627', 'FP_bit_628', 'FP_bit_629', 'FP_bit_630', 'FP_bit_631', 'FP_bit_632', 'FP_bit_633', 'FP_bit_634', 'FP_bit_635', 'FP_bit_636', 'FP_bit_637', 'FP_bit_638', 'FP_bit_639', 'FP_bit_640', 'FP_bit_641', 'FP_bit_642', 'FP_bit_643', 'FP_bit_644', 'FP_bit_645', 'FP_bit_646', 'FP_bit_647', 'FP_bit_648', 'FP_bit_649', 'FP_bit_650', 'FP_bit_651', 'FP_bit_652', 'FP_bit_653', 'FP_bit_654', 'FP_bit_655', 'FP_bit_656', 'FP_bit_657', 'FP_bit_658', 'FP_bit_659', 'FP_bit_660', 'FP_bit_661', 'FP_bit_662', 'FP_bit_663', 'FP_bit_664', 'FP_bit_665', 'FP_bit_666', 'FP_bit_667', 'FP_bit_668', 'FP_bit_669', 'FP_bit_670', 'FP_bit_671', 'FP_bit_672', 'FP_bit_673', 'FP_bit_674', 'FP_bit_675', 'FP_bit_676', 'FP_bit_677', 'FP_bit_678', 'FP_bit_679', 'FP_bit_680', 'FP_bit_681', 'FP_bit_682', 'FP_bit_683', 'FP_bit_684', 'FP_bit_685', 'FP_bit_686', 'FP_bit_687', 'FP_bit_688', 'FP_bit_689', 'FP_bit_690', 'FP_bit_691', 'FP_bit_692', 'FP_bit_693', 'FP_bit_694', 'FP_bit_695', 'FP_bit_696', 'FP_bit_697', 'FP_bit_698', 'FP_bit_699', 'FP_bit_700', 'FP_bit_701', 'FP_bit_702', 'FP_bit_703', 'FP_bit_704', 'FP_bit_705', 'FP_bit_706', 'FP_bit_707', 'FP_bit_708', 'FP_bit_709', 'FP_bit_710', 'FP_bit_711', 'FP_bit_712', 'FP_bit_713', 'FP_bit_714', 'FP_bit_715', 'FP_bit_716', 'FP_bit_717', 'FP_bit_718', 'FP_bit_719', 'FP_bit_720', 'FP_bit_721', 'FP_bit_722', 'FP_bit_723', 'FP_bit_724', 'FP_bit_725', 'FP_bit_726', 'FP_bit_727', 'FP_bit_728', 'FP_bit_729', 'FP_bit_730', 'FP_bit_731', 'FP_bit_732', 'FP_bit_733', 'FP_bit_734', 'FP_bit_735', 'FP_bit_736', 'FP_bit_737', 'FP_bit_738', 'FP_bit_739', 'FP_bit_740', 'FP_bit_741', 'FP_bit_742', 'FP_bit_743', 'FP_bit_744', 'FP_bit_745', 'FP_bit_746', 'FP_bit_747', 'FP_bit_748', 'FP_bit_749', 'FP_bit_750', 'FP_bit_751', 'FP_bit_752', 'FP_bit_753', 'FP_bit_754', 'FP_bit_755', 'FP_bit_756', 'FP_bit_757', 'FP_bit_758', 'FP_bit_759', 'FP_bit_760', 'FP_bit_761', 'FP_bit_762', 'FP_bit_763', 'FP_bit_764', 'FP_bit_765', 'FP_bit_766', 'FP_bit_767', 'FP_bit_768', 'FP_bit_769', 'FP_bit_770', 'FP_bit_771', 'FP_bit_772', 'FP_bit_773', 'FP_bit_774', 'FP_bit_775', 'FP_bit_776', 'FP_bit_777', 'FP_bit_778', 'FP_bit_779', 'FP_bit_780', 'FP_bit_781', 'FP_bit_782', 'FP_bit_783', 'FP_bit_784', 'FP_bit_785', 'FP_bit_786', 'FP_bit_787', 'FP_bit_788', 'FP_bit_789', 'FP_bit_790', 'FP_bit_791', 'FP_bit_792', 'FP_bit_793', 'FP_bit_794', 'FP_bit_795', 'FP_bit_796', 'FP_bit_797', 'FP_bit_798', 'FP_bit_799', 'FP_bit_800', 'FP_bit_801', 'FP_bit_802', 'FP_bit_803', 'FP_bit_804', 'FP_bit_805', 'FP_bit_806', 'FP_bit_807', 'FP_bit_808', 'FP_bit_809', 'FP_bit_810', 'FP_bit_811', 'FP_bit_812', 'FP_bit_813', 'FP_bit_814', 'FP_bit_815', 'FP_bit_816', 'FP_bit_817', 'FP_bit_818', 'FP_bit_819', 'FP_bit_820', 'FP_bit_821', 'FP_bit_822', 'FP_bit_823', 'FP_bit_824', 'FP_bit_825', 'FP_bit_826', 'FP_bit_827', 'FP_bit_828', 'FP_bit_829', 'FP_bit_830', 'FP_bit_831', 'FP_bit_832', 'FP_bit_833', 'FP_bit_834', 'FP_bit_835', 'FP_bit_836', 'FP_bit_837', 'FP_bit_838', 'FP_bit_839', 'FP_bit_840', 'FP_bit_841', 'FP_bit_842', 'FP_bit_843', 'FP_bit_844', 'FP_bit_845', 'FP_bit_846', 'FP_bit_847', 'FP_bit_848', 'FP_bit_849', 'FP_bit_850', 'FP_bit_851', 'FP_bit_852', 'FP_bit_853', 'FP_bit_854', 'FP_bit_855', 'FP_bit_856', 'FP_bit_857', 'FP_bit_858', 'FP_bit_859', 'FP_bit_860', 'FP_bit_861', 'FP_bit_862', 'FP_bit_863', 'FP_bit_864', 'FP_bit_865', 'FP_bit_866', 'FP_bit_867', 'FP_bit_868', 'FP_bit_869', 'FP_bit_870', 'FP_bit_871', 'FP_bit_872', 'FP_bit_873', 'FP_bit_874', 'FP_bit_875', 'FP_bit_876', 'FP_bit_877', 'FP_bit_878', 'FP_bit_879', 'FP_bit_880', 'FP_bit_881', 'FP_bit_882', 'FP_bit_883', 'FP_bit_884', 'FP_bit_885', 'FP_bit_886', 'FP_bit_887', 'FP_bit_888', 'FP_bit_889', 'FP_bit_890', 'FP_bit_891', 'FP_bit_892', 'FP_bit_893', 'FP_bit_894', 'FP_bit_895', 'FP_bit_896', 'FP_bit_897', 'FP_bit_898', 'FP_bit_899', 'FP_bit_900', 'FP_bit_901', 'FP_bit_902', 'FP_bit_903', 'FP_bit_904', 'FP_bit_905', 'FP_bit_906', 'FP_bit_907', 'FP_bit_908', 'FP_bit_909', 'FP_bit_910', 'FP_bit_911', 'FP_bit_912', 'FP_bit_913', 'FP_bit_914', 'FP_bit_915', 'FP_bit_916', 'FP_bit_917', 'FP_bit_918', 'FP_bit_919', 'FP_bit_920', 'FP_bit_921', 'FP_bit_922', 'FP_bit_923', 'FP_bit_924', 'FP_bit_925', 'FP_bit_926', 'FP_bit_927', 'FP_bit_928', 'FP_bit_929', 'FP_bit_930', 'FP_bit_931', 'FP_bit_932', 'FP_bit_933', 'FP_bit_934', 'FP_bit_935', 'FP_bit_936', 'FP_bit_937', 'FP_bit_938', 'FP_bit_939', 'FP_bit_940', 'FP_bit_941', 'FP_bit_942', 'FP_bit_943', 'FP_bit_944', 'FP_bit_945', 'FP_bit_946', 'FP_bit_947', 'FP_bit_948', 'FP_bit_949', 'FP_bit_950', 'FP_bit_951', 'FP_bit_952', 'FP_bit_953', 'FP_bit_954', 'FP_bit_955', 'FP_bit_956', 'FP_bit_957', 'FP_bit_958', 'FP_bit_959', 'FP_bit_960', 'FP_bit_961', 'FP_bit_962', 'FP_bit_963', 'FP_bit_964', 'FP_bit_965', 'FP_bit_966', 'FP_bit_967', 'FP_bit_968', 'FP_bit_969', 'FP_bit_970', 'FP_bit_971', 'FP_bit_972', 'FP_bit_973', 'FP_bit_974', 'FP_bit_975', 'FP_bit_976', 'FP_bit_977', 'FP_bit_978', 'FP_bit_979', 'FP_bit_980', 'FP_bit_981', 'FP_bit_982', 'FP_bit_983', 'FP_bit_984', 'FP_bit_985', 'FP_bit_986', 'FP_bit_987', 'FP_bit_988', 'FP_bit_989', 'FP_bit_990', 'FP_bit_991', 'FP_bit_992', 'FP_bit_993', 'FP_bit_994', 'FP_bit_995', 'FP_bit_996', 'FP_bit_997', 'FP_bit_998', 'FP_bit_999', 'FP_bit_1000', 'FP_bit_1001', 'FP_bit_1002', 'FP_bit_1003', 'FP_bit_1004', 'FP_bit_1005', 'FP_bit_1006', 'FP_bit_1007', 'FP_bit_1008', 'FP_bit_1009', 'FP_bit_1010', 'FP_bit_1011', 'FP_bit_1012', 'FP_bit_1013', 'FP_bit_1014', 'FP_bit_1015', 'FP_bit_1016', 'FP_bit_1017', 'FP_bit_1018', 'FP_bit_1019', 'FP_bit_1020', 'FP_bit_1021', 'FP_bit_1022', 'FP_bit_1023'])
    return solvent_desc

solvent_desc_clean = clean_data(solvent_desc.copy())


# %%
# solvents is the solvents column in the polystyrene subset. 
# Note that they are separated by a semicolon not comma, this avoids issues where some solvents name include commas.

solvents = pd.DataFrame(data_clean['Solvents'])

#solvents = solvents.merge(solvent_desc, on='Solvents', how="right")

# %%
solvents

# %%
# I find the Data Wrangler handy. Here I used it's functionality to split the solvents column based on the ; delimiter.
# And also drop outlier pairs when pairs of three or four solvents were used together.

"""
Cell generated by Data Wrangler.
"""
import pandas as pd

def clean_data(solvents):
    # Split text using string ';' in column: 'Solvents'
    loc_0 = solvents.columns.get_loc('Solvents')
    solvents_split = solvents['Solvents'].str.split(pat=';', expand=True).add_prefix('Solvents_')
    solvents = pd.concat([solvents.iloc[:, :loc_0], solvents_split, solvents.iloc[:, loc_0:]], axis=1)
    solvents = solvents.drop(columns=['Solvents'])
    # Filter rows based on column: 'Solvents_2'
    solvents = solvents[solvents['Solvents_2'].isna()]
    # Drop column: 'Solvents_3'
    solvents = solvents.drop(columns=['Solvents_3'])
    # Drop column: 'Solvents_2'
    solvents = solvents.drop(columns=['Solvents_2'])
    return solvents

solvents_clean = clean_data(solvents.copy())
#solvents_clean.head()

# %%
# Two columns and 174 rows matched my manual comparison using the original excel data sheet and excel filtering.

solvents_clean

# %%
# Here data wrangler was used again,
# This time to drop drop any outlier solvent entries that were a single solvent case - like 100% hexane.

"""
Cell generated by Data Wrangler.
"""
def clean_data(solvents_clean):
    # Filter rows based on column: 'Solvents_1'
    solvents_clean = solvents_clean[solvents_clean['Solvents_1'].notna()]
    return solvents_clean

solvents_clean_1 = clean_data(solvents_clean.copy())
solvents_clean_1.head()

# %%
# The solvents_clean_1 was then split into two separate data frames. 
# Each dataframe contains only one solvent while solvents_clean_1 was the pairs. 

combined_descriptors_1 = pd.DataFrame(solvents_clean_1['Solvents_0'])
combined_descriptors_2 = pd.DataFrame(solvents_clean_1['Solvents_1'])



# %%
# Note that the out lier dropping only removed 4 entries. Dropping from 176 rows to 172.

combined_descriptors_1

# %%
# Note that the out lier dropping only removed 4 entries. Dropping from 176 rows to 172.
combined_descriptors_2

# %%
# This was Data Wrangler removing the " (near crit)" that was present in a few niche solvent entries. 
# It is dropped and not tracked - hopefully no major affect to model performance. But not sure how to incorporate (near crit) properties.

"""
Cell generated by Data Wrangler.
"""
def clean_data(combined_descriptors_2):
    # Replace all instances of " (near crit)" with "" in column: 'Solvents_1'
    combined_descriptors_2['Solvents_1'] = combined_descriptors_2['Solvents_1'].str.replace(" (near crit)", "", case=False, regex=False)
    return combined_descriptors_2

combined_descriptors_2_clean = clean_data(combined_descriptors_2.copy())
combined_descriptors_2_clean.head()

# %%
# Utilizes pandas merge function to append the solvents' molecular descriptors onto the end of the data frames containing single solvents. 
# Index of the original solvent data is maintained and only descriptors with matching names to the names are appended to rows.

combined_descriptors1 = combined_descriptors_1.merge(solvent_desc_clean, left_on = 'Solvents_0', right_on = 'Solvents', how='left') 


# %%
combined_descriptors1

# %%
# This was needed to remove some white space that was trailing before or after a solvents name in the Solvents_1 dataframe 
# Note that this is the second solvent in the pair. 
# And also that all solvents were arranged so that the higher percentage ratio is the first solvent (Solvents_0) - This was done when the commas were switched to semicolons.

"""
Cell generated by Data Wrangler.
"""
combined_descriptors2 = pd.DataFrame()
def clean_data(combined_descriptors2):
    # Remove leading and trailing whitespace in column: 'Solvents_1'
    combined_descriptors2['Solvents_1'] = combined_descriptors_2_clean['Solvents_1'].str.strip()
    return combined_descriptors2

combined_descriptors2_clean = clean_data(combined_descriptors_2_clean.copy())
combined_descriptors2_clean

# %%
# Upon review commenting, I don't think this is necessary and is a artifact from me hunting the trailing white-space. Leaving it for now. 
combined_descriptors_2['Solvents_1'].str.strip()

# %%
# Utilizes pandas merge function to append the solvents' molecular descriptors onto the end of the data frames containing single solvents. 
# Index of the original solvent data is maintained and only descriptors with matching names to the names are appended to rows.

combined_descriptors2 = combined_descriptors2_clean.merge(solvent_desc_clean, left_on = 'Solvents_1', right_on = 'Solvents', how='left') 

# %%
combined_descriptors2

# %%
# Here the solvents have the name from the original data, followed by the name that matched in the RDkit excel sheet and the smiles from the Rdkit Excel sheet.
# All three of these values are strings and are dropped when creating the descriptors1 and descriptors2. These are only numerical values for the pairs of solvents. 
# The columns match and the index of pairing is preserved. 

descriptors1 = combined_descriptors1.drop(columns=['Solvents_0', 'Solvents', 'Smiles'])
descriptors2 = combined_descriptors2.drop(columns=['Solvents_1', 'Solvents', 'Smiles'])

# %%
descriptors1

# %%
descriptors2

# %%
# Pandas allows df1 + df2 to do cell by cell addition. 

combined_descriptors = descriptors1 + descriptors2

# %%
combined_descriptors

# %%
# These following lines, I just manually verified the addition was done properly.

#descriptors1.head()

# %%
#descriptors2.head()

# %%
#combined_descriptors.head()

# %%
# All the dataframes had their indexes reset to ensure concatenation of the descriptors to the end of the pairing dataframe was done in order.


solvents_clean_1_reset = solvents_clean_1.reset_index(drop=True)
combined_descriptors_reset = combined_descriptors.reset_index(drop=True)
processed_data = pd.concat([solvents_clean_1_reset, combined_descriptors_reset], axis=1)

# %%
combined_descriptors_reset

# %%
processed_data

# %%
# This begins the processing of the original data to match the preprocessing done to separate only solvent pairs.
# Note currently, 176 rows because it contains the single solvent, and pairs of 3 and 4 solvents.


data_clean = data_clean.reset_index(drop=True)
data_clean

# %%
#============================================================================
# Helper function to parse multi-value text columns
# (for Solvents, we keep up to 'max_splits' in order)
#============================================================================
def parse_multivalue_text(cell_value, max_splits=4):
    """
    Parse a comma-separated list of text (e.g., solvents).
    If fewer entries than max_splits, fill the rest with None.
    If more, truncate.
    """
    if pd.isnull(cell_value) or str(cell_value).strip() == "":
        return [None] * max_splits
    
    parts = [p.strip() for p in str(cell_value).split(";")]
    if len(parts) < max_splits:
        parts += [None] * (max_splits - len(parts))
    else:
        parts = parts[:max_splits]
    return parts

# %%
# I wanted to track the number of detectors - thought being that maybe I could incorporate it quality scoring rather than outright dropping the strings.

#============================================================================
# Detector handling 
#============================================================================
if "Detector" in data_clean.columns:
    data_clean["Detector"] = data_clean["Detector"].fillna("None")
    data_clean["Number_of_Detectors"] = data_clean["Detector"].apply(
        lambda x: 0 if x.strip().lower() == "none" else len(x.split(","))
    )

data_clean.drop("Detector", axis=1, inplace=True)

# %%
#============================================================================
# Parse the 'Solvents' column into 'Solvents_1', 'Solvents_2', ...
#============================================================================
if "Solvents" in data_clean.columns:
    max_solvents = 4  # or however many you want
    parsed_solvents = data_clean["Solvents"].apply(
        lambda x: parse_multivalue_text(x, max_splits=max_solvents)
    )
    solvent_cols = [f"Solvents_{i+1}" for i in range(max_solvents)]
    parsed_solvents_df = pd.DataFrame(parsed_solvents.tolist(), columns=solvent_cols)
    
    data_clean.drop(columns=["Solvents"], inplace=True)
    data_clean = pd.concat([data_clean, parsed_solvents_df], axis=1)

# %%
#============================================================================
# Helper function to parse multi-value numeric columns
#============================================================================
def parse_multivalue_numeric(cell_value, max_splits=4):
    """
    Parse a comma-separated list of numeric values.
    - If there's only 1 value, the rest become NaN
    - If more than max_splits, truncate
    """
    if pd.isnull(cell_value) or str(cell_value).strip() == "":
        return [np.nan] * max_splits
    
    parts = str(cell_value).split(",")
    floats = []
    for part in parts:
        part = part.strip()
        if part == "":
            floats.append(np.nan)
        else:
            try:
                floats.append(float(part))
            except ValueError:
                floats.append(np.nan)
    
    if len(floats) < max_splits:
        floats += [np.nan] * (max_splits - len(floats))
    else:
        floats = floats[:max_splits]
    
    return floats

# %%
#============================================================================
# Parse the 'Solvent Ratio' column (up to 4 numeric values)
#============================================================================
if "Solvent Ratio" in data_clean.columns:
    parsed_ratios = data_clean["Solvent Ratio"].apply(
        lambda x: parse_multivalue_numeric(x, max_splits=4)
    )
    ratio_cols = [f"Solvent Ratio_{i+1}" for i in range(4)]
    parsed_ratios_df = pd.DataFrame(parsed_ratios.tolist(), columns=ratio_cols)
    
    data_clean.drop(columns=["Solvent Ratio"], inplace=True)
    data_clean = pd.concat([data_clean, parsed_ratios_df], axis=1)

# %%
#============================================================================
# Parse 'Particle diameter (μm)' and 'Pore size (°A)' (up to 2 numeric values)
# We treat "no second entry" as simply np.nan for the second column.
#============================================================================
particle_pore_cols = {
    "Particle diameter (μm)": 2,
    "Pore size (°A)": 2
}

for col, max_splits in particle_pore_cols.items():
    if col in data_clean.columns:
        parsed_values = data_clean[col].apply(
            lambda x: parse_multivalue_numeric(x, max_splits=max_splits)
        )
        new_col_names = [f"{col}_{i+1}" for i in range(max_splits)]
        parsed_df = pd.DataFrame(parsed_values.tolist(), columns=new_col_names)
        
        data_clean.drop(columns=[col], inplace=True)
        data_clean = pd.concat([data_clean, parsed_df], axis=1)

# %%
data_clean

# %%
# Filter rows based on columns: 'Solvents_3', 'Solvents_2'
# Used below when dropping.
"""
Cell generated by Data Wrangler.
"""
def clean_data(data_clean):
    # Filter rows based on columns: 'Solvents_3', 'Solvents_2'
    data_clean = data_clean[(data_clean['Solvents_3'].isna()) & (data_clean['Solvents_2'].notna())]
    return data_clean

data_clean_1 = clean_data(data_clean.copy())
#data_clean_1.head()

# %%
data_clean_1

# %%
# This uses the above filtering to only drop solvents outlier pairs of 3 and 4 solvents.
# Which gets use to 172 rows matching, the preprocessing done to append solvents descriptors.
"""
Cell generated by Data Wrangler.
"""
def clean_data(data_clean_1):
    # Drop column: 'Solvents_3'
    data_clean_1 = data_clean_1.drop(columns=['Solvents_3'])
    # Drop column: 'Solvents_4'
    data_clean_1 = data_clean_1.drop(columns=['Solvents_4'])
    # Drop column: 'Solvent Ratio_3'
    data_clean_1 = data_clean_1.drop(columns=['Solvent Ratio_3'])
    # Drop column: 'Solvent Ratio_4'
    data_clean_1 = data_clean_1.drop(columns=['Solvent Ratio_4'])
    return data_clean_1

data_clean_2 = clean_data(data_clean_1.copy())
data_clean_2

# %%


# %%
processed_data

# %%
# All dataframes had their indexes reset and then the dataframes were concatenated so that the experimental data was followed by the descriptors.

data_clean_2_reset = data_clean_2.reset_index(drop=True)
processed_data_reset = processed_data.reset_index(drop=True)
final = pd.concat([data_clean_2_reset, processed_data_reset], axis=1)


# %%
final

# %%
# Removed the duplicate solvent names that came with the concatenation of descriptors and experimental data.

"""
Cell generated by Data Wrangler.
"""
def clean_data(final):
    # Drop column: 'Solvent Ratio_2'
    final = final.drop(columns=['Solvent Ratio_2'])
    # Drop column: 'Solvents_0'
    final = final.drop(columns=['Solvents_0'])
    final = final.loc[:,~final.columns.duplicated()].copy()
    return final

final_clean = clean_data(final.copy())
#final_clean.head()

# %%
final_clean

# %%
# All missing temperatures were filled with 25.

"""
Cell generated by Data Wrangler.
"""
def clean_data(final_clean):
    # Replace missing values with 25 in column: 'Temperature (Celsius)'
    final_clean = final_clean.fillna({'Temperature (Celsius)': 25})
    return final_clean

imputated = clean_data(final_clean.copy())


# %%
imputated

# %%
# Now hot encoding and imputation before model training
# Utilizes scikit-learns OneHotEncoder for categorical experimental data related to columns.
# Utilizes scikit-learns IterativeImputer - with xgboost current but I've tried a few regressors for imputation. 

from sklearn.preprocessing import OneHotEncoder

from sklearn.experimental import enable_iterative_imputer  
from sklearn.impute import IterativeImputer

from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import fetch_openml

#import cupy as cp  # <-- CuPy for GPU arrays

import xgboost as xgb
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials

# %%
#============================================================================
# One-hot encode other single-valued categorical columns (e.g. Polymer, etc.)
#============================================================================
other_categorical_cols = [
    #"Polymer",
    "Stationary Phase",
    "Base Material",
    "Base Material Modification",
    "Phase",
]

other_categorical_cols = [c for c in other_categorical_cols if c in imputated.columns]

# Fill missing with "None" for these columns too
for c in other_categorical_cols:
    imputated[c] = imputated[c].fillna("None")

ohe_other = OneHotEncoder(sparse_output=False, drop='first')
cat_df = imputated[other_categorical_cols].astype(str)
encoded_array_other = ohe_other.fit_transform(cat_df)

encoded_feature_names_other = ohe_other.get_feature_names_out(other_categorical_cols)
encoded_other_df = pd.DataFrame(encoded_array_other, columns=encoded_feature_names_other)

# Drop original categorical columns
imputated.drop(columns=other_categorical_cols, inplace=True)

# Combine
imputated = pd.concat([imputated.reset_index(drop=True), encoded_other_df], axis=1)

# %%
# Define the XGBoost Regressor
xgb_regressor = xgb.XGBRegressor(

    tree_method="hist",
    device="cuda",
    n_estimators=500,
    max_depth=6,
    learning_rate=0.01,
    verbosity=2,
    random_state=42
)

# %%
# Iterative Imputer will fill any missing values - 
# Therefore Pore size 2 and particle diameter 2 if non existent were filled with 0 to be included in the data used for imputation of missing data.

imputated['Pore size (°A)_2'].fillna(0, inplace=True)
imputated['Particle diameter (μm)_2'].fillna(0, inplace=True)

# %%
imputated

# %%
# 
impute_cols = [
    "Temperature (Celsius)",
    "Flow Rate (mL/min)",
    "Used Sample Range Low (kD)",
    "Used Sample Range High (kD)",
    "Critical Range Low (kD)",
    "Critical Range High (kD)",
    "Particle diameter (μm)_1",
    "Pore size (°A)_1",
    "Injection Volume",
    # Above are the columns which should be imputed and filled,
    # Below are columns already full and should be used in the prediction of the other columns. 
    
    # The columns like Pore size _2 or Particle diameter_2 are not used in the imputation in this version as I am not
    # whether it is better to include a sentiental place holder for intentional empty values like -9999 or just not prediction using those features.

    "Quality Score",
    "Number of Columns",
    "Number_of_Detectors",
    "Solvent Ratio_1",

	"Stationary Phase_Alltima C18",	"Stationary Phase_Asahipak GF-310 HQ",	"Stationary Phase_Atlantis C18",	"Stationary Phase_Bakerbond C18",	"Stationary Phase_Betasil Si",	"Stationary Phase_Biospher Si",	"Stationary Phase_Biospher Si, Separon SGX-1000",	"Stationary Phase_Bridged Ethane Hybrid particle C18",	"Stationary Phase_C18",	"Stationary Phase_C18, Nucleosil C18",	"Stationary Phase_Chrompak RP-18",	"Stationary Phase_Develosil NH2",	"Stationary Phase_Discovery C18",	"Stationary Phase_Discovery Cyano",	"Stationary Phase_Discovery HS-PEG",	"Stationary Phase_Equisil C18",	"Stationary Phase_Eurospher II NH2",	"Stationary Phase_Hypercarb",	"Stationary Phase_Hypersil C18",	"Stationary Phase_Hypersil Si",	"Stationary Phase_Inertsil Platinum Si",	"Stationary Phase_Inertsil WP300 C18",	"Stationary Phase_Jordi Gel DVB",	"Stationary Phase_Jordi Gel DVB 500 RP",	"Stationary Phase_Jordi Gel DVB 500 RP",	"Stationary Phase_Jupiter C18",	"Stationary Phase_Juptier C18",	"Stationary Phase_Kromasil C18",	"Stationary Phase_LiChrosorb Diol",	"Stationary Phase_LiChrosorb Si",	"Stationary Phase_LiChrospher NH2", "Stationary Phase_LiChrospher Si",	"Stationary Phase_Luna C18",	"Stationary Phase_Luna HILIC",	"Stationary Phase_Lunasil C18",	"Stationary Phase_Lunasil Si",	"Stationary Phase_Macroporous glass",	"Stationary Phase_Novapak C18",	"Stationary Phase_Nucelosil C18",	"Stationary Phase_Nucleodur C18 Gravity",	"Stationary Phase_Nucleodur C18 Pyramid",	"Stationary Phase_Nucleosil C18",	"Stationary Phase_Nucleosil C18 AB",	"Stationary Phase_Nucleosil C18, C18",	"Stationary Phase_Nucleosil C8 HD, Nucleosil C8",	"Stationary Phase_Nucleosil CN",	"Stationary Phase_Nucleosil Diol",	"Stationary Phase_Nucleosil NH2",	"Stationary Phase_Nucleosil Si",	"Stationary Phase_Onyx Monolithic C18",	"Stationary Phase_PLRP-S", 	"Stationary Phase_PLgel PS/DVB gel",	"Stationary Phase_PS/DVB gel",	"Stationary Phase_PS/DVB gel Eurogel PRP 100",	"Stationary Phase_Perfectsil Si",	"Stationary Phase_PhenoSphere-NEXT Si",	"Stationary Phase_Platinum Si",	"Stationary Phase_Porasil Si E+D", "Stationary Phase_Prodigy ODS-3",	"Stationary Phase_Prontosil C18",	"Stationary Phase_Resolve C18",	"Stationary Phase_Resolve Si",	"Stationary Phase_S5X Si",	"Stationary Phase_Separon SAX",	"Stationary Phase_Separon SGX",	"Stationary Phase_Si",	"Stationary Phase_Si 5SIL 20E",	"Stationary Phase_Si NH2",	"Stationary Phase_Silasorb 600",	"Stationary Phase_Silasorb Si",	"Stationary Phase_Silasorb Si SPH 600",	"Stationary Phase_Soloza K-0",	"Stationary Phase_Soloza K-33",	"Stationary Phase_Soloza KG-8",	"Stationary Phase_Sphereclone C6",	"Stationary Phase_Spherisorb ODS2",	"Stationary Phase_Spherisorb S5P",	"Stationary Phase_Spherisorb S5X C6", 	"Stationary Phase_Supelco C18",	"Stationary Phase_Symmetry C18",	"Stationary Phase_Symmetry C18, Nucleosil C18",	"Stationary Phase_Synergi Fusion RP",	"Stationary Phase_Synergi MAX RP",	"Stationary Phase_TLC KSK Si",	"Stationary Phase_TLC KSK-2 Si",	"Stationary Phase_TLC KSKG Si",	"Stationary Phase_TLC LiChrospher",	"Stationary Phase_TLC Si",	"Stationary Phase_TSKgel ODS-80Ts QA",	"Stationary Phase_Toyopearl Butyl-650M",	"Stationary Phase_Ultisil Diol",	"Stationary Phase_Ultisil XB-Phenyl",	"Stationary Phase_XB-C18",	"Stationary Phase_YMC C18",	"Stationary Phase_YMC RP",	"Stationary Phase_YMC-ODSA", 	"Stationary Phase_YMC-Pack Si SIL AP",	"Stationary Phase_Zorbax 300 C18",	"Stationary Phase_Zorbax Eclipse XDB-C18",	"Stationary Phase_Zorbax Eclipse XDB-C18, Pack ODS-A",	"Stationary Phase_Zorbax ODS",	"Stationary Phase_Zorbax Rx-C18",	"Stationary Phase_Zorbax SB C18",	"Stationary Phase_Zorbax-Gold", "Stationary Phase_Zorbax-NH2",	"Stationary Phase_μBondapak NH2",	"Stationary Phase_μStyragel HT",	"Base Material_DVB",	"Base Material_None",	"Base Material_PS/DVB",	"Base Material_PVA",	"Base Material_Porous Graphitic Carbon",	"Base Material_Silica", "Base Material Modification_C18",	"Base Material Modification_C4",	"Base Material Modification_C6",	"Base Material Modification_C8",	"Base Material Modification_CN",	"Base Material Modification_Cross-linked OH",	"Base Material Modification_NH2",	"Base Material Modification_No Mod",	"Base Material Modification_No Modifications",	"Base Material Modification_OH", "Base Material Modification_PEG",	"Base Material Modification_Phenyl",	"Base Material Modification_Unknown",	"Phase_Ion Exchange",	"Phase_None",	"Phase_Normal",	"Phase_Reverse"



# Sorry that I listed out all the columns I want to use in the iterationImputer rather than removing the ones I didn't want to use. Something didn't work right with that approach.


# Here are the previous hot encoded manufacturer columns - However, this version of code drops manufacturers before imputation. So I am commentting out here. 
# Maybe I should include in imputation - I'll have to run this then run with hot encoded manufacturer columns for better imputation before dropping when it comes to the actual ML model ? or I'll include, IDK.

#"Manufacturer_Agilent Technologies",	"Manufacturer_Agilent Technologies", 	"Manufacturer_Agilent, YMC",	"Manufacturer_Alltech",	"Manufacturer_Alltech, Nucleosil,", "Manufacturer_Avantor",	"Manufacturer_Chrompack",	"Manufacturer_Cluzeau",	"Manufacturer_Delta-Pak",	"Manufacturer_Elsiko",	"Manufacturer_GL Science",	"Manufacturer_J.T. Baker",	"Manufacturer_Jordi",	"Manufacturer_Jordi", 	"Manufacturer_Keystone Scientific",	"Manufacturer_Knauer",	"Manufacturer_Kromasil",	"Manufacturer_LabAlliance",	"Manufacturer_Labio",	"Manufacturer_MZ Analysentechnik",	"Manufacturer_Macherey-Nagel",	"Manufacturer_Macherey-Nagel", 	"Manufacturer_Macherey-Nagel + VWR",	"Manufacturer_Merck",	"Manufacturer_Nomura Chemical", 	"Manufacturer_None",	"Manufacturer_Nova-Pak",	"Manufacturer_Phase Separations",	"Manufacturer_Phase Separations", 	"Manufacturer_PhaseSep",	"Manufacturer_Phenogel",	"Manufacturer_Phenomenex",	"Manufacturer_Phenomenex", 	"Manufacturer_Polymer Laboratories",	"Manufacturer_Reakhim",	"Manufacturer_SR-8700 Chromatography",	"Manufacturer_Shandon",	"Manufacturer_Shodex",	"Manufacturer_Styragel",	"Manufacturer_Supelco",	"Manufacturer_Tessek",	"Manufacturer_ThermoFisher Scientific",	"Manufacturer_ThermoQuest", "Manufacturer_Tosoh",	"Manufacturer_Toya Soda",	"Manufacturer_Waters",	"Manufacturer_Waters Atlantis",	"Manufacturer_Waters, Supelco",	"Manufacturer_Welch",	"Manufacturer_YMC",	"Manufacturer_μBondagel",
    
]

# %%
# Note that this is all the columns - if its full it is purely used to predict missing values in other columns.
impute_cols = [c for c in impute_cols if c in imputated.columns]

# Prints in the order I wrote the columns to be used with the Iterative Imputer.
print("Columns to be imputed with IterativeImputer:", impute_cols)

# %%
# Create a copy of your data to store the imputed results
data_imputed = imputated.copy()

# Subset the columns we want to impute  --- probably not necessary on this version. 
subset_df = imputated[impute_cols]

# %%
# The columns must all be numeric (float or int). If they're not, 
# check or convert them; otherwise, IterativeImputer will raise an error.






# Note - default estimator for Imputer is BayesianRidge() when not defined.

# Note - default imputation_order parameter is ascending meaning imputer starts on the columns with the least missing data first - Known to be most efficient for most datasets.
#       However, we may want to try imputation_order set to random -(When to use:
#                                                                   If there’s no clear relationship between missing values in features.
#                                                                   When you want to reduce potential biases from fixed orders.)


imputer = IterativeImputer(estimator=xgb_regressor, random_state=42, max_iter=100,tol=1e-2, verbose=2)
imputed_array = imputer.fit_transform(subset_df)

# Put the imputed values back into data_imputed
data_imputed[impute_cols] = imputed_array

# %%
# Now 'data_imputed' has imputed values for the columns in 'impute_cols', 

final_output_file = "polystyrene-imputated-molecular-descriptor-includes-inj-volume-2-15-25.xlsx"
data_imputed.to_excel(final_output_file, index=False)

# Saves and prints the file to be used in the ML model.
print(f"Saved data with iterative imputation to {final_output_file}")

# %%



